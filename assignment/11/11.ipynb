{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Multi-label classification using neural networks with a regularization\n",
    "* Writer : Jesoon Kang, Chung-Ang University\n",
    "* last-modified date : June 4, 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import math\n",
    "from IPython.display import display, Math, Latex\n",
    "import os\n",
    "import time\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.datasets import load_files\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data = load_files(r\"movie_review\")\n",
    "X, y = review_data.data, review_data.target\n",
    "\n",
    "documents = []\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for sen in range(0, len(X)):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
    "    \n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "    \n",
    "    documents.append(document)\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=10000, min_df=5, max_df=0.5, stop_words=stopwords.words('english'))\n",
    "X = vectorizer.fit_transform(documents).toarray()\n",
    "\n",
    "tfidfconverter = TfidfTransformer()\n",
    "X = tfidfconverter.fit_transform(X).toarray()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1401, 10000) (601, 10000) (1401,) (601,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "GeForce RTX 2060 SUPER\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "### Setting up Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Below codes activates when want to use cpu\n",
    "#device = torch.device('cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_grad(z):\n",
    "    sig_value =  1.0 / (1.0 + np.exp(-z))\n",
    "    return sig_value * (1 - sig_value)\n",
    "\n",
    "# Return derivative of in\n",
    "def sig_grad(z) :\n",
    "    return z * (1-z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to getting running time\n",
    "def get_running_time(start_time) :\n",
    "    running_time = datetime.datetime.now() - start_time\n",
    "    running_time = running_time.seconds\n",
    "    hours, remainder = divmod(running_time, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    return hours, minutes, seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = {}\n",
    "\n",
    "def preprocess_data(max_features, min_df, max_df) :\n",
    "\n",
    "    review_data = load_files(r\"movie_review\")\n",
    "    X, y = review_data.data, review_data.target\n",
    "\n",
    "    documents = []\n",
    "\n",
    "    stemmer = WordNetLemmatizer()\n",
    "\n",
    "    for sen in range(0, len(X)):\n",
    "        # Remove all the special characters\n",
    "        document = re.sub(r'\\W', ' ', str(X[sen]))\n",
    "\n",
    "        # remove all single characters\n",
    "        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "        # Remove single characters from the start\n",
    "        document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "\n",
    "        # Substituting multiple spaces with single space\n",
    "        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "\n",
    "        # Removing prefixed 'b'\n",
    "        document = re.sub(r'^b\\s+', '', document)\n",
    "\n",
    "        # Converting to Lowercase\n",
    "        document = document.lower()\n",
    "\n",
    "        # Lemmatization\n",
    "        document = document.split()\n",
    "        document = [stemmer.lemmatize(word) for word in document]\n",
    "        document = ' '.join(document)\n",
    "\n",
    "        documents.append(document)\n",
    "\n",
    "    vectorizer = CountVectorizer(max_features=max_features, min_df=min_df, max_df=max_df, stop_words=stopwords.words('english'))\n",
    "    X = vectorizer.fit_transform(documents).toarray()\n",
    "\n",
    "    tfidfconverter = TfidfTransformer()\n",
    "    X = tfidfconverter.fit_transform(X).toarray()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=False)\n",
    "    # Make image, label data to tensor type\n",
    "    train_input = X_train.T\n",
    "    train_label = y_train\n",
    "    test_input = X_test.T\n",
    "    test_label = y_test\n",
    "    train_label_vec = train_label\n",
    "    test_label_vec = test_label\n",
    "    train_input_with_bias = np.ones((train_input.shape[0]+1, train_input.shape[1])) # add bia\n",
    "    train_input_with_bias[1:, :] = train_input\n",
    "    test_input_with_bias = np.ones((test_input.shape[0]+1, test_input.shape[1])) # add bia\n",
    "    test_input_with_bias[1:, :] = test_input\n",
    "\n",
    "    # Make data to tensor\n",
    "    data[\"train_input_with_bias\"] = torch.DoubleTensor(train_input_with_bias).to(device)\n",
    "    data[\"test_input_with_bias\"] = torch.DoubleTensor(test_input_with_bias).to(device)\n",
    "    data[\"train_label_vec\"] = torch.DoubleTensor(train_label_vec).to(device)\n",
    "    data[\"test_label_vec\"] = torch.DoubleTensor(test_label_vec).to(device)\n",
    "    data[\"train_input\"] = torch.DoubleTensor(train_input).to(device)\n",
    "    data[\"test_input\"] = torch.DoubleTensor(test_input).to(device)\n",
    "    data[\"train_label\"] = torch.DoubleTensor(train_label).to(device)\n",
    "    data[\"test_label\"] = torch.DoubleTensor(test_label).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make image, label data to tensor type\n",
    "train_image = X_train.T\n",
    "train_label = y_train\n",
    "test_image = X_test.T\n",
    "test_label = y_test\n",
    "train_label_vec = train_label\n",
    "test_label_vec = test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_with_bias = np.ones((train_image.shape[0]+1, train_image.shape[1])) # add bia\n",
    "train_input_with_bias[1:, :] = train_image\n",
    "test_input_with_bias = np.ones((test_image.shape[0]+1, test_image.shape[1])) # add bia\n",
    "test_input_with_bias[1:, :] = test_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make data to tensor\n",
    "train_input_with_bias = torch.DoubleTensor(train_input_with_bias).to(device)\n",
    "test_input_with_bias = torch.DoubleTensor(test_input_with_bias).to(device)\n",
    "train_label_vec = torch.DoubleTensor(train_label_vec).to(device)\n",
    "test_label_vec = torch.DoubleTensor(test_label_vec).to(device)\n",
    "train_label = torch.DoubleTensor(train_label).to(device)\n",
    "test_label = torch.DoubleTensor(test_label).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class train_classifier :\n",
    "    def __init__(self,data, layer_info = [196,49,10],lr = 1e-3,weight_decay= 1e-4,loss_conv = 1e-6,monitoring_epoch = 0) :\n",
    "        print('-'*20,\"ML START\", '-'*20)\n",
    "        \n",
    "        \n",
    "        self.feature_size = data[\"train_input\"].shape[0]\n",
    "        \n",
    "        print(\"lr : {}, weight_decay : {}, loss_conv : {}, thetas : {}\".format(lr, weight_decay, loss_conv, layer_info))\n",
    "        self.list_layer_info = layer_info\n",
    "        self.init_thetas()\n",
    "        # Get number of thetas(total)\n",
    "        self.th_num = 0\n",
    "        for th in self.list_theta :\n",
    "            self.th_num += th.view(-1).shape[0]\n",
    "        \n",
    "        # Initial variable setting\n",
    "        self.monitoring_epoch = monitoring_epoch\n",
    "        self.num_hidden_layer = len(self.list_theta)-1\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.loss_conv = loss_conv\n",
    "        self.epoch = 0\n",
    "        self.list_epoch = []\n",
    "        self.list_loss_train = []\n",
    "        self.list_loss_test = []\n",
    "        self.list_pred_train = []\n",
    "        self.list_pred_test = []\n",
    "        self.list_acc_train = []\n",
    "        self.list_acc_test = []\n",
    "        self.list_y = []\n",
    "        self.list_a = []\n",
    "        self.result_train = {}\n",
    "        self.result_test = {}\n",
    "        self.start_time = datetime.datetime.now()\n",
    "        self.exit_count = 0\n",
    "        self.list_dL = []\n",
    "        self.list_grad = []\n",
    "        self.exit_count = 0\n",
    "        self.top_acc = 0\n",
    "        self.train()\n",
    "\n",
    "    # Get mean value of theta square\n",
    "    def get_mean_theta_square(self,list_theta) :\n",
    "        sum_th = 0\n",
    "        th_num = 0\n",
    "        for th in list_theta :\n",
    "            sum_th += torch.sum(th**2)\n",
    "            th_num += th.view(-1).shape[0]\n",
    "        return sum_th/self.th_num\n",
    "    \n",
    "    # Initialize theta\n",
    "    def init_thetas(self) :\n",
    "        self.list_theta = []\n",
    "        list_d = []\n",
    "        list_d.append(self.feature_size)\n",
    "        for d in self.list_layer_info :\n",
    "            th = torch.randn((d,list_d[-1]+1),dtype=torch.double).to(device)\n",
    "            self.list_theta.append(th)\n",
    "            list_d.append(d)\n",
    "    \n",
    "        \n",
    "    # monitoring \n",
    "    def monitoring(self) :\n",
    "        if self.monitoring_epoch != 0 :\n",
    "            if self.epoch % self.monitoring_epoch == 0 :\n",
    "                running_time = get_running_time(self.start_time)\n",
    "                print(\"Train Acc : {:.3f}, Loss : {:.8f} | epoch : {}, time : {:02d}:{:02d}:{:02d}\\nTest  Acc : {:.3f}, Loss : {:.8f}\".\\\n",
    "                      format(self.list_acc_train[-1],self.list_loss_train[-1],self.epoch,running_time[0],running_time[1],running_time[2],self.list_acc_test[-1],self.list_loss_test[-1]))\n",
    "        \n",
    "    # Train.\n",
    "    def train(self) :\n",
    "        while (True) :\n",
    "            self.forward_propagation_train()\n",
    "            self.set_gradient_decent()\n",
    "            self.forward_propagation_test()\n",
    "            self.update_weights()\n",
    "            self.monitoring()\n",
    "            self.check_top_acc()\n",
    "            if self.check_terminate() :\n",
    "                self.terminate()\n",
    "                break\n",
    "        \n",
    "\n",
    "    # Check the loss gap\n",
    "    def check_terminate(self) :\n",
    "        if self.epoch > 5 :\n",
    "            loss_gap = abs(self.list_loss_train[-1] - self.list_loss_train[-2])\n",
    "            if loss_gap < self.loss_conv :\n",
    "                self.exit_count += 1\n",
    "            else :\n",
    "                self.exit_count = 0\n",
    "            if self.exit_count > 4 :\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    # Terminate Train. Save example data\n",
    "    def terminate(self) :\n",
    "        # Save result.\n",
    "        #####################\n",
    "        self.top_result()\n",
    "        ######################\n",
    "        self.result_train[\"epoch\"] = self.list_epoch\n",
    "        self.result_train[\"loss\"] = self.list_loss_train\n",
    "        self.result_train[\"acc\"] = self.list_acc_train\n",
    "        self.result_train[\"theta\"] = self.list_theta\n",
    "\n",
    "        self.result_test[\"epoch\"] = self.list_epoch\n",
    "        self.result_test[\"loss\"] = self.list_loss_test\n",
    "        self.result_test[\"acc\"] = self.list_acc_test\n",
    "\n",
    "       \n",
    "        \n",
    "        print(\"Loss is converged.\")\n",
    "        print(\"Training Process Ended\")\n",
    "        return True\n",
    "        \n",
    "    # Forward propagation on train data\n",
    "    def forward_propagation_train(self) :\n",
    "        self.epoch += 1\n",
    "        self.list_epoch.append(self.epoch)\n",
    "        self.list_y = []\n",
    "        self.list_a = []\n",
    "        self.list_a_bias = [data[\"train_input_with_bias\"]]\n",
    "        for idx, th in enumerate(self.list_theta) :    \n",
    "            y_tmp = torch.mm(th,self.list_a_bias[-1])\n",
    "            self.list_y.append(y_tmp)\n",
    "            a_tmp = 1 / (1 + torch.exp(-y_tmp))\n",
    "            self.list_a.append(a_tmp)\n",
    "            a_bias_tmp = torch.ones((a_tmp.shape[0] + 1, a_tmp.shape[1]), dtype=torch.double).to(device)\n",
    "            a_bias_tmp[1:, :] = a_tmp\n",
    "            self.list_a_bias.append(a_bias_tmp)\n",
    "        self.list_a_bias.pop()\n",
    "        \n",
    "        h = self.list_a[-1]\n",
    "        \n",
    "        # Get Loss value with L2 regulazation\n",
    "        loss_train = torch.sum(-data[\"train_label_vec\"] * (torch.log(h)) - (1 - data[\"train_label_vec\"])* torch.log(1 - h)) / len(h.T) + self.weight_decay*0.5*self.get_mean_theta_square(self.list_theta)\n",
    "        self.list_loss_train.append(loss_train)\n",
    "        \n",
    "        self.pred_train = (h >=0.5)\n",
    "        self.list_pred_train.append(self.pred_train)\n",
    "        \n",
    "        acc_train = accuracy_score(data[\"train_label\"].cpu(),self.pred_train.squeeze(0).cpu())\n",
    "        self.list_acc_train.append(acc_train)\n",
    "    \n",
    "    def predict(self,input_data) :\n",
    "        input_data = input_data\n",
    "        if type(input_data) == np.ndarray :\n",
    "            None\n",
    "        elif type(input_data) == torch.Tensor :\n",
    "            input_data = input_data.cpu().numpy()\n",
    "        else :\n",
    "            return -1\n",
    "        \n",
    "        input_with_bias = np.ones((input_data.shape[0]+1, input_data.shape[1])) # add bia\n",
    "        input_with_bias[1:, :] = input_data\n",
    "        input_with_bias = torch.DoubleTensor(input_with_bias).to(device)\n",
    "        list_y = []\n",
    "        list_a = []\n",
    "        list_a_bias = [input_with_bias]\n",
    "        \n",
    "        for idx, th in enumerate(self.list_theta) :\n",
    "            y_tmp = torch.mm(th,list_a_bias[-1])\n",
    "            list_y.append(y_tmp)\n",
    "            a_tmp = 1 / (1 + torch.exp(-y_tmp))\n",
    "            list_a.append(a_tmp)\n",
    "            a_bias_tmp = torch.ones((a_tmp.shape[0] + 1, a_tmp.shape[1]), dtype=torch.double).to(device)\n",
    "            a_bias_tmp[1:, :] = a_tmp\n",
    "            list_a_bias.append(a_bias_tmp)\n",
    "\n",
    "        h = list_a[-1]\n",
    "        \n",
    "        pred = h >= 0.5\n",
    "        if pred.shape[0] == 1 :\n",
    "            pred = pred.squeeze(0)\n",
    "        \n",
    "        return pred.cpu()\n",
    "    \n",
    "    # Forward propagation in test data\n",
    "    def forward_propagation_test(self) :\n",
    "        \n",
    "        self.list_y = []\n",
    "        self.list_a = []\n",
    "        self.list_a_bias = [data[\"test_input_with_bias\"]]\n",
    "        for idx, th in enumerate(self.list_theta) :\n",
    "           \n",
    "            y_tmp = torch.mm(th,self.list_a_bias[-1])\n",
    "            self.list_y.append(y_tmp)\n",
    "            a_tmp = 1 / (1 + torch.exp(-y_tmp))\n",
    "            self.list_a.append(a_tmp)\n",
    "            a_bias_tmp = torch.ones((a_tmp.shape[0] + 1, a_tmp.shape[1]), dtype=torch.double).to(device)\n",
    "            a_bias_tmp[1:, :] = a_tmp\n",
    "            self.list_a_bias.append(a_bias_tmp)\n",
    "        self.list_a_bias.pop()\n",
    "        \n",
    "        h = self.list_a[-1]\n",
    "        \n",
    "        # Get Loss value with L2 regulazation\n",
    "        loss_test = torch.sum(-data[\"test_label_vec\"] * (torch.log(h)) - (1 - data[\"test_label_vec\"])* torch.log(1 - h)) / len(h.T) + self.weight_decay*0.5*self.get_mean_theta_square(self.list_theta)\n",
    "        self.list_loss_test.append(loss_test)\n",
    "\n",
    "\n",
    "        # Calculate Acc\n",
    "    \n",
    "        self.pred_test = h >= 0.5\n",
    "        self.list_pred_test.append(self.pred_test)\n",
    "        acc_test = accuracy_score(data[\"test_label\"].cpu(),self.pred_test.squeeze(0).cpu())\n",
    "        self.list_acc_test.append(acc_test)\n",
    "        \n",
    "    def set_gradient_decent (self) :\n",
    "         # Back propagation\n",
    "        self.list_dL = []\n",
    "        self.list_grad = []\n",
    "        \n",
    "        self.list_dL.append(self.list_a[-1]-train_label_vec)\n",
    "        grad = torch.mm(self.list_dL[-1],self.list_a_bias[-1].T) + self.weight_decay*0.5*self.list_theta[-1]/self.th_num\n",
    "        \n",
    "        self.list_grad.append(torch.mm(self.list_dL[-1],self.list_a_bias[-1].T))\n",
    "        idx = 0\n",
    "        # Calculate gradient decent in order from back-end\n",
    "        for i in range(0,self.num_hidden_layer) :\n",
    "            idx += 1\n",
    "            dL_dy = torch.mm(self.list_theta[-idx].T,self.list_dL[-1]) * sig_grad(self.list_a_bias[-idx])\n",
    "            dL_dy = dL_dy[1:, :]\n",
    "            self.list_dL.append(dL_dy)\n",
    "           \n",
    "            # Get gradient decent value Applying L2 Regulazation \n",
    "            grad = torch.mm(dL_dy, self.list_a_bias[-(idx+1)].T)\n",
    "            grad_2 = self.weight_decay*self.list_theta[-(idx+1)]#/self.th_num\n",
    "            grad = grad + grad_2\n",
    "            self.list_grad.append(grad)\n",
    "        self.list_grad.reverse()\n",
    "        \n",
    "        # Update weights \n",
    "    def update_weights(self) :\n",
    "        for th, grad in zip(self.list_theta,self.list_grad) :\n",
    "            th -= self.lr * (grad)\n",
    " \n",
    "    def show_loss_curve(self) :\n",
    "        plt.title(\"Loss curve\")\n",
    "        plot_1, = plt.plot(self.result_train[\"epoch\"],self.result_train[\"loss\"], color='b',linewidth=2,alpha=0.8)\n",
    "        plot_2, = plt.plot(self.result_test[\"epoch\"],self.result_test[\"loss\"], color='r',linewidth=2,alpha=0.8)\n",
    "        plt.legend([plot_1,plot_2],[\"Training\",\"Testing\"])\n",
    "\n",
    "    def show_acc_curve(self) :\n",
    "        plt.title(\"Accuracy curve\")\n",
    "        plot_1, = plt.plot(self.result_train[\"epoch\"],self.result_train[\"acc\"], color='b',linewidth=2,alpha=0.8)\n",
    "        plot_2, = plt.plot(self.result_test[\"epoch\"],self.result_test[\"acc\"], color='r',linewidth=2,alpha=0.8)\n",
    "        plt.legend([plot_1,plot_2],[\"Training\",\"Testing\"])\n",
    "\n",
    "    def show_final_acc(self) :\n",
    "        print(\"Final Training Acc : {:.3f}%\\nFinal Testing Acc : {:.3f}%\".format(self.result_train[\"acc\"][-1]*100,self.result_test[\"acc\"][-1]*100))\n",
    "\n",
    "    def show_final_test_acc(self) :\n",
    "        print(\"Final Testing Acc : {:f}%\".format(self.result_test[\"acc\"][-1]*100))\n",
    "\n",
    "\n",
    "        \n",
    "    def check_top_acc(self) :\n",
    "        if (self.list_acc_test[-1]>0.82) :\n",
    "            if (self.list_acc_test[-1] > self.top_acc) :\n",
    "                self.top_acc = self.list_acc_test[-1]\n",
    "                self.top_acc_epoch = self.list_epoch[-1]\n",
    "                self.top_theta = []\n",
    "                for i in self.list_theta :\n",
    "                    self.top_theta.append(i.detach())\n",
    "                \n",
    "                #print(\"epoch : {}, Acc reach the top : {}\".format(self.list_epoch[-1],self.top_acc))\n",
    "    def show_top_acc(self) :\n",
    "        print(\"{}, epoch : {}\".format(self.top_acc,self.top_acc_epoch))\n",
    "    def show_train_results(self) :\n",
    "        y_pred_train = self.predict(data[\"train_input\"])\n",
    "        y_train = data[\"train_label\"].cpu()\n",
    "        print(\"#\"*10,\"Confusion Matrix\",\"#\"*10)\n",
    "        print(confusion_matrix(y_train,y_pred_train))\n",
    "        print(\"#\"*10,\"Classification_Report\",\"#\"*10)\n",
    "        print(classification_report(y_train,y_pred_train))\n",
    "        print(\"#\"*10,\"Accuracy Score\",\"#\"*10)\n",
    "        print(accuracy_score(y_train, y_pred_train))\n",
    "    def show_test_results(self) :\n",
    "        y_pred_test = self.predict(data[\"test_input\"])\n",
    "        y_test = data[\"test_label\"].cpu()\n",
    "        print(\"#\"*10,\"Confusion Matrix\",\"#\"*10)\n",
    "        print(confusion_matrix(y_test,y_pred_test))\n",
    "        print(\"#\"*10,\"Classification_Report\",\"#\"*10)\n",
    "        print(classification_report(y_test,y_pred_test))\n",
    "        print(\"#\"*10,\"Accuracy Score\",\"#\"*10)\n",
    "        print(accuracy_score(y_test,y_pred_test))\n",
    "        \n",
    "    def top_result(self) :\n",
    "        if self.top_acc_epoch :\n",
    "            self.list_epoch = self.list_epoch[:self.top_acc_epoch]\n",
    "            self.list_loss_train = self.list_loss_train[:self.top_acc_epoch]\n",
    "            self.list_acc_train = self.list_acc_train[:self.top_acc_epoch]\n",
    "            self.list_loss_test = self.list_loss_test[:self.top_acc_epoch]\n",
    "            self.list_acc_test = self.list_acc_test[:self.top_acc_epoch]\n",
    "            self.list_theta = self.top_theta\n",
    "            self.epoch = self.top_acc_epoch\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_data(11000,5,0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- ML START --------------------\n",
      "lr : 0.0001, weight_decay : 0.8, loss_conv : 1e-07, thetas : [2, 1]\n"
     ]
    }
   ],
   "source": [
    "best_classifier1 = None\n",
    "for i in range (0,10) :\n",
    "    classifier = train_classifier(data,layer_info=[2,1],lr= 1e-4,loss_conv = 1e-7,weight_decay=8e-1,monitoring_epoch=0)\n",
    "    if best_classifier1 == None :\n",
    "        best_classifier1 = classifier\n",
    "        print(\"Best classifier is updated\")\n",
    "        print(\"Final acc : {}, top acc : {}\".format(best_classifier1.result_test[\"acc\"][-1],best_classifier1.top_acc))\n",
    "    else :\n",
    "        if (best_classifier1.result_test[\"acc\"][-1] < classifier.result_test[\"acc\"][-1]) :\n",
    "            best_classifier1 = classifier\n",
    "            print(\"Best classifier is updated\")\n",
    "            print(\"Final acc : {}, top acc : {}\".format(best_classifier1.result_test[\"acc\"][-1],best_classifier1.top_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_classifier2 = None\n",
    "\n",
    "for i in range (0,10) :\n",
    "    classifier = train_classifier(data,layer_info=[2,1],lr= 1e-4,loss_conv = 1e-7,weight_decay=15e-1,monitoring_epoch=0)\n",
    "    if best_classifier2 == None :\n",
    "        best_classifier2 = classifier\n",
    "        print(\"Best classifier is updated\")\n",
    "        print(\"Final acc : {}, top acc : {}\".format(best_classifier2.result_test[\"acc\"][-1],best_classifier2.top_acc))\n",
    "    else :\n",
    "        if (best_classifier2.result_test[\"acc\"][-1] < classifier.result_test[\"acc\"][-1]) :\n",
    "            best_classifier2 = classifier\n",
    "            print(\"Best classifier is updated\")\n",
    "            print(\"Final acc : {}, top acc : {}\".format(best_classifier2.result_test[\"acc\"][-1],best_classifier2.top_acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_classifier3 = None\n",
    "for i in range (0,10) :\n",
    "    classifier = train_classifier(data,layer_info=[2,1],lr= 1e-4,loss_conv = 1e-7,weight_decay=20e-1,monitoring_epoch=0)\n",
    "    if best_classifier3 == None :\n",
    "        best_classifier3 = classifier\n",
    "        print(\"Best classifier is updated\")\n",
    "        print(\"Final acc : {}, top acc : {}\".format(best_classifier3.result_test[\"acc\"][-1],best_classifier3.top_acc))\n",
    "    else :\n",
    "        if (best_classifier3.result_test[\"acc\"][-1] < classifier.result_test[\"acc\"][-1]) :\n",
    "            best_classifier3 = classifier\n",
    "            print(\"Best classifier is updated\")\n",
    "            print(\"Final acc : {}, top acc : {}\".format(best_classifier3.result_test[\"acc\"][-1],best_classifier3.top_acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.show_top_acc()\n",
    "classifier.show_final_test_acc()\n",
    "classifier.show_test_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.show_loss_curve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the accuracy curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.show_acc_curve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the quantitative results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.show_train_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.show_test_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.show_final_test_acc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_classifier = None\n",
    "best_acc = None\n",
    "best_acc_epoch = None\n",
    "\n",
    "\n",
    "\n",
    "for i in range(5,40) :\n",
    "    preprocess_data(i*500,5,0.8)\n",
    "    print(\"*\"*50)\n",
    "    print(\"Feature size : \",data[\"train_input\"].shape[0])\n",
    "    print(\"*\"*50)\n",
    "\n",
    "    for j in range(0,3) :\n",
    "        classifier = train_classifier(data,layer_info=[20,12,1],lr= 1e-3,loss_conv = 1e-7,weight_decay=4e-1,monitoring_epoch=0)\n",
    "        classifier.show_final_acc()\n",
    "        classifier.show_top_acc()\n",
    "        if not best_classifier :\n",
    "            best_classifier = classifier\n",
    "            best_acc = classifier.top_acc\n",
    "            best_acc_epoch = classifier.top_acc_epoch\n",
    "            print(\"#\"*50)\n",
    "            print(\"Best classifier is updated.\\nFeature : {}, Acc : {}\".format(data[\"train_input\"].shape[0],best_acc,best_acc_epoch))\n",
    "            print(\"#\"*50)\n",
    "            continue\n",
    "\n",
    "        if best_classifier.top_acc < classifier.top_acc :\n",
    "            best_classifier = classifier\n",
    "            best_acc = classifier.top_acc\n",
    "            best_acc_epoch = classifier.top_acc_epoch\n",
    "            print(\"#\"*100)\n",
    "            print(\"Best classifier is updated.\\nFeature : {}, Acc : {}\".format(data[\"train_input\"].shape[0],best_acc,best_acc_epoch))\n",
    "            print(\"#\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preprocess_data(11000,5,0.8)\n",
    "classifier = train_classifier(data,layer_info=[50,20,12,1],lr= 1e-3,loss_conv = 1e-7,weight_decay=4e-1,monitoring_epoch=500)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
